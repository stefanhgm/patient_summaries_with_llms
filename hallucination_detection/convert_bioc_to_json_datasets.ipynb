{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create JSON datasets from raw bioc labelings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from utils import read_bioc, parse_text_labels\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define files and parameters\n",
    "data_path = '/home/s_hegs02/MedTator'\n",
    "data_path = Path(data_path)\n",
    "\n",
    "dataset_paths = {\n",
    "    # Experiment 1: label mimic summaries\n",
    "    'hallucinations_100_mimic_annotator_1': data_path / '10_label_silver_examples_annotator_1' / 'hallucinations_100_mimic_annotator_1.xml',\n",
    "    'hallucinations_100_mimic_annotator_2': data_path / '11_label_silver_examples_annotator_2' / 'hallucinations_100_mimic_annotator_2.xml',\n",
    "    'hallucinations_100_mimic_agreed': data_path / '12_agreed_label_silver_examples' / 'hallucinations_100_mimic_agreed.xml',\n",
    "    'hallucinations_10_valid_mimic_agreed': data_path / '13_agreed_label_silver_validation_examples' / 'hallucinations_10_valid_mimic_agreed.xml',\n",
    "    # Experiment 2: label generated summaries\n",
    "    'hallucinations_100_generated_annotator_1': data_path / '20_label_halus_qualitative_annotator_1' / 'hallucinations_100_generated_annotator_1.xml',\n",
    "    'hallucinations_100_generated_annotator_2': data_path / '21_label_halus_qualitative_annotator_2' / 'hallucinations_100_generated_annotator_2.xml',\n",
    "    'hallucinations_100_generated_agreed': data_path / '22_label_halus_qualitative_agreed' / 'hallucinations_100_generated_agreed.xml',\n",
    "}\n",
    "\n",
    "# Randomization for Experiment 2\n",
    "hallucination_random_models = {0: [0, 4, 3, 1, 2], 1: [4, 1, 3, 2, 0], 2: [0, 2, 1, 3, 4], 3: [1, 0, 3, 4, 2], 4: [3, 2, 4, 1, 0], 5: [3, 2, 1, 4, 0], 6: [1, 2, 3, 0, 4], 7: [1, 3, 2, 4, 0], 8: [4, 0, 1, 3, 2], 9: [0, 3, 2, 4, 1], 10: [0, 4, 3, 2, 1], 11: [1, 0, 4, 3, 2], 12: [2, 4, 1, 0, 3], 13: [3, 1, 0, 4, 2], 14: [4, 2, 0, 1, 3], 15: [0, 2, 4, 3, 1], 16: [1, 4, 2, 3, 0], 17: [2, 3, 1, 0, 4], 18: [4, 0, 3, 2, 1], 19: [0, 3, 1, 2, 4], 20: [4, 0, 2, 3, 1], 21: [0, 4, 2, 1, 3], 22: [0, 2, 4, 3, 1], 23: [1, 0, 3, 4, 2], 24: [3, 1, 0, 4, 2], 25: [2, 0, 3, 4, 1], 26: [4, 3, 0, 1, 2], 27: [3, 4, 2, 1, 0], 28: [4, 2, 3, 1, 0], 29: [4, 1, 3, 0, 2], 30: [2, 3, 0, 1, 4], 31: [4, 2, 0, 3, 1], 32: [3, 0, 2, 1, 4], 33: [2, 3, 4, 1, 0], 34: [4, 1, 3, 2, 0], 35: [0, 4, 1, 3, 2], 36: [4, 1, 3, 0, 2], 37: [3, 1, 0, 4, 2], 38: [3, 2, 4, 1, 0], 39: [1, 0, 3, 4, 2], 40: [4, 3, 0, 1, 2], 41: [2, 3, 4, 0, 1], 42: [2, 4, 3, 1, 0], 43: [4, 1, 2, 0, 3], 44: [0, 4, 3, 1, 2], 45: [3, 2, 0, 1, 4], 46: [2, 4, 0, 3, 1], 47: [2, 1, 0, 4, 3], 48: [4, 2, 3, 1, 0], 49: [3, 1, 4, 2, 0]}\n",
    "\n",
    "# Define markers\n",
    "re_text_start_mimic_old_key = re.compile('### JSON Key: text\\n', re.MULTILINE)\n",
    "re_summary_start_mimic_old_key = re.compile('### JSON Key: summary\\n', re.MULTILINE)\n",
    "re_text_start_mimic = re.compile('Text:\\n', re.MULTILINE)\n",
    "re_summary_start_mimic = re.compile('Summary:\\n', re.MULTILINE)\n",
    "re_text_start_generated = re.compile('Text:\\n', re.MULTILINE)\n",
    "re_summary_start_generated = re.compile('Summary \\d:\\n', re.MULTILINE)\n",
    "markers = {k: (re_text_start_mimic, re_summary_start_mimic) if 'mimic' in k else (re_text_start_generated, re_summary_start_generated) for k in dataset_paths.keys()}\n",
    "# This two medtator datasets still used the old key\n",
    "markers['hallucinations_100_mimic_annotator_1'] = (re_text_start_mimic_old_key, re_summary_start_mimic_old_key)\n",
    "markers['hallucinations_100_mimic_annotator_2'] = (re_text_start_mimic_old_key, re_summary_start_mimic_old_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_bioc = {k: read_bioc(v) for k, v in dataset_paths.items()}\n",
    "datasets_unprocessed = {k: parse_text_labels(v) for k, v in datasets_bioc.items()}\n",
    "datasets = {k: [] for k in datasets_unprocessed.keys()}\n",
    "\n",
    "# Print included ids\n",
    "for k, v in datasets_unprocessed.items():\n",
    "    print(f\"{k}: {len(v)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The datasets still contain the text (BHC) and the summary (discharge instructions) and the label positions are based on both texts.\n",
    "# Additionally, the generated examples contain one text and 5 randomized generations\n",
    "# Must split this data and correct the label positions to be based on the summaries only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 100 examples with 239 labels to hallucinations_100_mimic_annotator_1\n",
      "Added 100 examples with 282 labels to hallucinations_100_mimic_annotator_2\n",
      "Added 100 examples with 286 labels to hallucinations_100_mimic_agreed\n",
      "Added 10 examples with 23 labels to hallucinations_10_valid_mimic_agreed\n"
     ]
    }
   ],
   "source": [
    "# 1. Labeling: First split text and summaries in mimic examples and correct label positions\n",
    "\n",
    "for dataset_name in [k for k in datasets_unprocessed.keys() if 'mimic' in k]:\n",
    "    # print(k)\n",
    "    # Get all keys sorted\n",
    "    sorted_keys = (list(datasets_unprocessed[dataset_name].keys()))\n",
    "    sorted_keys.sort()\n",
    "    for key in sorted_keys:\n",
    "        # print(key)\n",
    "        # print(example)\n",
    "        re_text_start, re_summary_start = markers[dataset_name]\n",
    "        example = datasets_unprocessed[dataset_name][key]\n",
    "        text_start = re_text_start.search(example['text']).span()[1]\n",
    "        text_end = re_summary_start.search(example['text']).span()[0]\n",
    "        summary_start = re_summary_start.search(example['text']).span()[1]\n",
    "\n",
    "        text = example['text'][text_start:text_end].strip()\n",
    "        summary = example['text'][summary_start:].rstrip()\n",
    "        assert len(summary.lstrip()) == len(summary)\n",
    "        # Debug\n",
    "        # print(text)\n",
    "        # print(summary)\n",
    "        \n",
    "        label_offset = summary_start\n",
    "        labels = []\n",
    "        for label in example['labels']:\n",
    "            new_label = label.copy()\n",
    "            new_label['start'] -= label_offset\n",
    "            new_label['end'] -= label_offset\n",
    "            # print(label, new_label)\n",
    "            # Verify correct label\n",
    "            assert example['text'][label['start']:label['end']] == label['text']\n",
    "            assert summary[new_label['start']:new_label['end']] == label['text']\n",
    "            labels.append(new_label)\n",
    "            \n",
    "        datasets[dataset_name].append({'text': text, 'summary': summary, 'labels': labels})\n",
    "    print(f\"Added {len(datasets[dataset_name])} examples with {sum([len(ex['labels']) for ex in datasets[dataset_name]])} labels to {dataset_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 100 examples with 123 labels to hallucinations_100_generated_annotator_1\n",
      "Added 100 examples with 118 labels to hallucinations_100_generated_annotator_2\n",
      "Added 100 examples with 114 labels to hallucinations_100_generated_agreed\n"
     ]
    }
   ],
   "source": [
    "# 2. Labeling: Second de-randomize generated summaries and put them into separate text-summary\n",
    "\n",
    "for dataset_name in [k for k in datasets_unprocessed.keys() if 'generated' in k]:\n",
    "\n",
    "    num_unfolded_generated_examples = 0\n",
    "    unfolded_generated_examples = [[] for i in range(5)]\n",
    "    for id, example in datasets_unprocessed[dataset_name].items():\n",
    "        # Get text entry between re_text_start_generated and re_summary_start_generated\n",
    "        source = example['text']\n",
    "        labels = example['labels']\n",
    "        text = source[re_text_start_generated.search(source).end():re_summary_start_generated.search(source).start()].strip()\n",
    "        summaries_labels = []\n",
    "        \n",
    "        source_summaries_offset = re_summary_start_generated.search(source).start()\n",
    "        summary_label_len = len('Summary X:\\n')\n",
    "        source_summaries = source[source_summaries_offset:]\n",
    "        # Get all positions of re_text_start_generated\n",
    "        summary_start_positions = [m.start() for m in re_summary_start_generated.finditer(source_summaries)] + [len(source_summaries)]\n",
    "        summary_lens = [summary_start_positions[i+1] - summary_start_positions[i] for i in range(5)]\n",
    "        \n",
    "        # print(summary_start_positions)\n",
    "        # for i in range(5):\n",
    "        #     print('---' + source_summaries[summary_start_positions[i] + summary_label_len:summary_start_positions[i+1]] + '---')\n",
    "        \n",
    "        randomized_summaries_labels = []\n",
    "        processed_labels = []\n",
    "        for i in range(5):\n",
    "            summary_content_start = source_summaries_offset + summary_start_positions[i] + summary_label_len\n",
    "            summary_content_end = source_summaries_offset + summary_start_positions[i+1]\n",
    "            summary = source[summary_content_start:summary_content_end]\n",
    "            summaries_labels = []\n",
    "\n",
    "            # Get all labels for this summary\n",
    "            for label in labels:\n",
    "                if label['start'] >= summary_content_start and label['end'] <= summary_content_end:\n",
    "                    # Verify labe;\n",
    "                    assert source[label['start']:label['end']] == label['text']\n",
    "                    # Copy label\n",
    "                    new_label = label.copy()\n",
    "                    # Correct the label position\n",
    "                    new_label['start'] = label['start'] - summary_content_start\n",
    "                    new_label['end'] = label['end'] - summary_content_start\n",
    "                    # Check label at correct position in extracted summary\n",
    "                    assert summary[new_label['start']:new_label['end']] == label['text']\n",
    "                    summaries_labels.append(new_label)\n",
    "                    processed_labels.append(label)\n",
    "            randomized_summaries_labels.append({'summary': summary, 'labels': summaries_labels})\n",
    "            \n",
    "        # Check that all labels were processed\n",
    "        assert processed_labels == labels\n",
    "        assert sum([len(ex['labels']) for ex in randomized_summaries_labels]) == len(labels)\n",
    "        # Check all cahracter of source were processed\n",
    "        assert source_summaries_offset + sum([len(ex['summary']) for ex in randomized_summaries_labels]) + 5 * summary_label_len == len(source)\n",
    "        # Now remove trailing whitespaces for summaries and chek no leading whitespaces\n",
    "        for i in range(5):\n",
    "            assert len(randomized_summaries_labels[i]['summary']) == len(randomized_summaries_labels[i]['summary'].lstrip())\n",
    "            randomized_summaries_labels[i]['summary'] = randomized_summaries_labels[i]['summary'].rstrip()\n",
    "            \n",
    "        # De-randomize\n",
    "        summaries_labels = [''] * 5\n",
    "        for i in range(5):\n",
    "            summaries_labels[hallucination_random_models[id][i]] = randomized_summaries_labels[i]\n",
    "        assert [e != '' for e in summaries_labels].count(True) == 5\n",
    "        \n",
    "        # Debug:\n",
    "        # for e in summaries_labels:\n",
    "        #     print(e['summary'])\n",
    "        #     print(e['labels'])\n",
    "        # print('---')\n",
    "        \n",
    "        # Move examples with text-summary format into unpacked\n",
    "        for i in range(5):\n",
    "            unfolded_generated_examples[i].append({'text': text, 'summary': summaries_labels[i]['summary'], 'labels': summaries_labels[i]['labels']})\n",
    "            num_unfolded_generated_examples += 1\n",
    "            \n",
    "    # Combine all lists into one\n",
    "    assert num_unfolded_generated_examples == 5 * len(datasets_unprocessed[dataset_name])\n",
    "    datasets[dataset_name] = unfolded_generated_examples[0] + unfolded_generated_examples[1] + unfolded_generated_examples[2] + unfolded_generated_examples[3] + unfolded_generated_examples[4]\n",
    "    print(f\"Added {len(datasets[dataset_name])} examples with {sum([len(ex['labels']) for ex in datasets[dataset_name]])} labels to {dataset_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out datasets as jsonl into same folders as original bioc files\n",
    "output_dir = Path('/home/s_hegs02/patient_summaries_with_llms')\n",
    "for dataset_name in datasets.keys():\n",
    "    file_name = str(dataset_paths[dataset_name]).split('/')[-1]\n",
    "    file_name = file_name.replace('.xml', '.jsonl')\n",
    "    with open(output_dir / file_name, 'w') as f:\n",
    "        for example in datasets[dataset_name]:\n",
    "            f.write(json.dumps(example) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avs_gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
