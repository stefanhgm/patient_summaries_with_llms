{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to Extract Medcat UMLS Entities and Determine Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from medcat.cat import CAT\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel  \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment name\n",
    "experiment_name = \"umls_large\"\n",
    "\n",
    "# Define files and parameters\n",
    "bioc_labelled_hallucinations_10_valid_mimic_summaries_path = '/home/s_hegs02/MedTator/13_agreed_label_silver_validation_examples/hallucinations_10_valid_mimic_agreed.jsonl'\n",
    "bioc_labelled_hallucinations_100_mimic_summaries_path = '/home/s_hegs02/MedTator/12_agreed_label_silver_examples/hallucinations_100_mimic_agreed.jsonl'\n",
    "# TODO: Replace with the agreed dataset\n",
    "bioc_labelled_hallucinations_100_generated_summaries = '/home/s_hegs02/MedTator/20_label_halus_qualitatative_annotator_1/hallucinations_100_generated_annotator_1.jsonl'\n",
    "dataset_paths = {'valid_mimic': bioc_labelled_hallucinations_10_valid_mimic_summaries_path, 'test_mimic': bioc_labelled_hallucinations_100_mimic_summaries_path, 'test_generated': bioc_labelled_hallucinations_100_generated_summaries}\n",
    "entities_output_path = \"/home/s_hegs02/mimic-iv-note-di-bhc/entities/\"\n",
    "\n",
    "\n",
    "# MedCat model\n",
    "# Small model: UMLS Small (A modelpack containing a subset of UMLS (disorders, symptoms, medications...). Trained on MIMIC-III)\n",
    "# cat_model_path = \"/home/s_hegs02/medcat/models/umls_sm_pt2ch.zip\"\n",
    "# Large model: UMLS Full. >4MM concepts trained self-supervsied on MIMIC-III. v2022AA of UMLS.\n",
    "cat_model_path = \"/home/s_hegs02/medcat/models/umls_self_train_model.zip\"\n",
    "num_cpus = 4\n",
    "\n",
    "# Semantic types of Griffin's \"What's in a Summary\" paper\n",
    "# Disorders, Chemicals & Drugs, Procedures semantic groups, Lab Results \n",
    "# See groups here: https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/SemGroups_2018.txt\n",
    "filtered_semantic_types = [\n",
    "    'T020', 'T190', 'T049', 'T019', 'T047', 'T050', 'T033', 'T037', 'T048', 'T191', 'T046', 'T184',\n",
    "    'T116', 'T195', 'T123', 'T122', 'T103', 'T120', 'T104', 'T200', 'T196', 'T126', 'T131', 'T125', 'T129', 'T130', 'T197', 'T114', 'T109', 'T121', 'T192', 'T127',\n",
    "    'T060', 'T065', 'T058', 'T059', 'T063', 'T062', 'T061', \n",
    "    'T034'\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "def read_jsonl(path):\n",
    "    input = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            input.append(json.loads(line))\n",
    "    return input\n",
    "\n",
    "datasets = {k: read_jsonl(v) for k, v in dataset_paths.items()}\n",
    "\n",
    "# Verify that all labels are correctly located\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    for i, doc in enumerate(dataset):\n",
    "        for label in doc['labels']:\n",
    "            assert label['start'] >= 0 and label['end'] <= len(doc['summary']), f\"Label {label} in dataset {dataset_name} is out of bounds for text of length {len(doc['summary'])} in document {i}\"\n",
    "            assert doc['summary'][label['start']:label['end']] == label['text'], f\"Label {label} in dataset {dataset_name} does not match text in document {i}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract entities for all concepts in the texts and summaries\n",
    "\n",
    "# Load medcat model\n",
    "cat = CAT.load_model_pack(cat_model_path)\n",
    "\n",
    "# Get entities for all texts and summaries in the datasets\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    output_file = Path(entities_output_path) / f\"medcat_entities_{dataset_name}_{experiment_name}.json\"\n",
    "\n",
    "    if output_file.exists():\n",
    "        print(f\"File {output_file} already exists. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Extracting medcat entities for dataset {dataset_name}...\")\n",
    "\n",
    "    # Load input json as pandas dataframe\n",
    "    df = pd.DataFrame(dataset)[['text', 'summary']]\n",
    "    assert df.notnull().values.all()\n",
    "\n",
    "    # Prepare input data to MedCat by extracting all texts into one list\n",
    "    i = 0\n",
    "    in_data = []\n",
    "    for col in df.columns:\n",
    "        for _, text in enumerate(df[col].values):\n",
    "            # Extract entities\n",
    "            in_data.append((i, text))\n",
    "            i += 1\n",
    "\n",
    "    # Perform concept extraction\n",
    "    # out_data is a dictionary for all input texts including a dictionay \"entities\" with all extracted entities for this text\n",
    "    out_data = cat.multiprocessing(in_data, nproc=num_cpus)\n",
    "    print(f'Total number of entities extracted: {sum([len(text[\"entities\"]) for text in out_data.values()])}')\n",
    "\n",
    "    # Count occurrences of semantic types in semantic_types in the extracted entities\n",
    "    semantic_types_counts = {}\n",
    "    for text in out_data.values():\n",
    "        for entity in text['entities'].values():\n",
    "            for semantic_type in entity['type_ids']:\n",
    "                if semantic_type in filtered_semantic_types:\n",
    "                    semantic_types_counts[semantic_type] = semantic_types_counts.get(semantic_type, 0) + 1\n",
    "    print(f'Number of entities per inculded semantic type:')\n",
    "    print({s: semantic_types_counts.get(s, 0) for s in filtered_semantic_types})\n",
    "\n",
    "    # Filter out entities that are not in the semantic types\n",
    "    for text in out_data.values():\n",
    "        text['entities'] = {idx: entity for idx, entity in text['entities'].items() if any([s in entity['type_ids'] for s in filtered_semantic_types])}\n",
    "    print(f'Total number of entities extracted after filtering: {sum([len(text[\"entities\"]) for text in out_data.values()])}')\n",
    "\n",
    "    # Write back all extracted entities into the same format as the input\n",
    "    i = 0\n",
    "    for col in df.columns:\n",
    "        for j, _ in enumerate(df[col].values):\n",
    "            df[col][j] = [out_data[i]]\n",
    "            i += 1\n",
    "\n",
    "    # Save output to json\n",
    "    df.to_json(output_file, orient='records', indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SapBERT model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\")  \n",
    "model = AutoModel.from_pretrained(\"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting sapbert embeddings for dataset valid_mimic...\n",
      "Total number of entities: 519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 13.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting sapbert embeddings for dataset test_mimic...\n",
      "Total number of entities: 4891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:01<00:00, 46.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting sapbert embeddings for dataset test_generated...\n",
      "Total number of entities: 4638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [00:01<00:00, 46.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings with SapBERT (Liu et al., 2021) for all extraxted entities\n",
    "\n",
    "# Get embeddings for all texts and summaries in the datasets\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    output_file = Path(entities_output_path) / f\"medcat_entities_sapbert_embeddings_{dataset_name}_{experiment_name}.json\"\n",
    "\n",
    "    if output_file.exists():\n",
    "        print(f\"File {output_file} already exists. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Extracting sapbert embeddings for dataset {dataset_name}...\")\n",
    "\n",
    "    medcat_file = Path(entities_output_path) / f\"medcat_entities_{dataset_name}_{experiment_name}.json\"\n",
    "    if not medcat_file.exists():\n",
    "        print(f\"File {medcat_file} does not exist. Skipping...\")\n",
    "        continue\n",
    "    entities = pd.read_json(medcat_file)\n",
    "    total_num_entities = sum([len(text[0]['entities']) for text in entities['text'].values]) + sum([len(text[0]['entities']) for text in entities['summary'].values])\n",
    "\n",
    "    # Extract all relevant text sections \n",
    "    all_pretty_names = []\n",
    "    all_source_values = []\n",
    "    for _, text in enumerate(entities['text'].values):\n",
    "        for _, entity in text[0]['entities'].items():\n",
    "            all_pretty_names.append(entity['pretty_name'])\n",
    "            all_source_values.append(entity['source_value'])\n",
    "    for _, text in enumerate(entities['summary'].values):\n",
    "        for _, entity in text[0]['entities'].items():\n",
    "            all_pretty_names.append(entity['pretty_name'])\n",
    "            all_source_values.append(entity['source_value'])\n",
    "            \n",
    "    assert total_num_entities == len(all_pretty_names) == len(all_source_values), f\"Total number of entities {total_num_entities} does not match the number of pretty names {len(all_pretty_names)} or source values {len(all_source_values)}\"\n",
    "    print(f\"Total number of entities: {total_num_entities}\")\n",
    "\n",
    "    # From: https://github.com/cambridgeltl/sapbert/blob/main/inference/inference_on_snomed.ipynb\n",
    "    bs = 128\n",
    "    all_names = all_pretty_names + all_source_values\n",
    "    all_reps = []\n",
    "    for i in tqdm(np.arange(0, len(all_names), bs)):\n",
    "        toks = tokenizer.batch_encode_plus(all_names[i:i+bs], \n",
    "                                        padding=\"max_length\", \n",
    "                                        max_length=25, \n",
    "                                        truncation=True,\n",
    "                                        return_tensors=\"pt\").to('cuda')\n",
    "        output = model(**toks)\n",
    "        cls_rep = output[0][:,0,:]\n",
    "        \n",
    "        all_reps.append(cls_rep.cpu().detach().numpy())\n",
    "    all_reps_emb = np.concatenate(all_reps, axis=0)\n",
    "\n",
    "    assert len(all_reps_emb) == len(all_names), f\"Number of embeddings {len(all_reps_emb)} does not match the number of names {len(all_names)}\"\n",
    "\n",
    "    all_pretty_names_emb = list(all_reps_emb[:len(all_pretty_names)])\n",
    "    all_source_values_emb = list(all_reps_emb[len(all_pretty_names):])\n",
    "\n",
    "    # Save embeddings with entities\n",
    "    for _, text in enumerate(entities['text'].values):\n",
    "        for _, entity in text[0]['entities'].items():\n",
    "            entity['pretty_name_embedding'] = all_pretty_names_emb.pop(0).tolist()\n",
    "            entity['source_value_embedding'] = all_source_values_emb.pop(0).tolist()\n",
    "    for _, text in enumerate(entities['summary'].values):\n",
    "        for _, entity in text[0]['entities'].items():\n",
    "            entity['pretty_name_embedding'] = all_pretty_names_emb.pop(0).tolist()\n",
    "            entity['source_value_embedding'] = all_source_values_emb.pop(0).tolist()\n",
    "\n",
    "    # Save output to json\n",
    "    entities.to_json(output_file, orient='records', indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avs_gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
