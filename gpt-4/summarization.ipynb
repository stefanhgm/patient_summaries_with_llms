{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-4 Summarization\n",
    "\n",
    "## Creating a Prompt\n",
    "\n",
    "For creating a prompt, I will give you 10 training examples of the original text-summary pairs and 10 validation examples.\n",
    "I will also provide code to check the performance of the 10 validation examples below.\n",
    "You have to imput the output of GPT-4 for these by hand.\n",
    "Not all experiments use exactly the same data as the original text-summary pairs (see below), but I think these are good to get a sense of the performance and create a prompt for all experiments.\n",
    "\n",
    "## Experiments To Run\n",
    "\n",
    "All other experiments come with their own 10 in-context examples.\n",
    "\n",
    "### For quantitative performance estimates\n",
    "\n",
    "1. Summarization of 100 original text-summary pairs\n",
    "2. Summarization of 100 original text-summary pairs with short text (<4000 chars) and long summaries (>600 chars)\n",
    "    * I did not mention this to you, but we also have to get the performance on this data.\n",
    "    * This is a subset of 20% of the data I had to work with to make the human annotation feasible. Too long texts where impossible to annotate.\n",
    "    * Basically I just want to show that this subselection makes no difference in performance.\n",
    "3. Not high priority, but could be useful: Summarization of 100 _cleaned and improved_ text-summary pairs when using 10 cleaned and improved in-context examples (10 validation _cleaned and improved data_)\n",
    "\n",
    "### For annotating hallucinations and determining hallucination rates\n",
    "\n",
    "4. Summarization of 25 examples when using in-context examples with unsupported facts (10 validation _original data_)\n",
    "    * I will give you 50 test examples to have some for debugging\n",
    "5. Summarization of 25 examples when using in-context examples with unsupported facts removed (10 validation _cleaned data_)\n",
    "    * I will give you 50 test examples to have some for debugging\n",
    "\n",
    "### For qualitative results with human annotation\n",
    "\n",
    "6. Summarization of 25 examples when using in-context examples with unsupported facts removed and improved text such as deidentification removed (10 validation _cleaned and improved data_)\n",
    "    * I will give you 50 test examples to have some for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import evaluate\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all files\n",
    "def read_jsonl(file_name):\n",
    "    with open(file_name, \"r\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "    \n",
    "prompt_train = read_jsonl('summarization_data/prompt_train.json')\n",
    "prompt_valid = read_jsonl('summarization_data/prompt_valid.json')\n",
    "\n",
    "exp_1_in_context = read_jsonl('summarization_data/exp_1_in-context.json')\n",
    "exp_1_test = read_jsonl('summarization_data/exp_1_test.json')\n",
    "exp_2_in_context = read_jsonl('summarization_data/exp_2_in-context.json')\n",
    "exp_2_test = read_jsonl('summarization_data/exp_2_test.json')\n",
    "exp_3_in_context = read_jsonl('summarization_data/exp_3_in-context.json')\n",
    "exp_3_test = read_jsonl('summarization_data/exp_3_test.json')\n",
    "\n",
    "exp_4_in_context = read_jsonl('summarization_data/exp_4_in-context.json')\n",
    "exp_4_test = read_jsonl('summarization_data/exp_4_test.json')\n",
    "exp_5_in_context = read_jsonl('summarization_data/exp_5_in-context.json')\n",
    "exp_5_test = read_jsonl('summarization_data/exp_5_test.json')\n",
    "\n",
    "exp_6_in_context = read_jsonl('summarization_data/exp_6_in-context.json')\n",
    "exp_6_test = read_jsonl('summarization_data/exp_6_test.json')\n",
    "\n",
    "assert len(prompt_train) == 10\n",
    "assert len(prompt_valid) == 10\n",
    "# Assert length of in-context always 10\n",
    "assert len(exp_1_in_context) == 10\n",
    "assert len(exp_2_in_context) == 10\n",
    "assert len(exp_3_in_context) == 10\n",
    "assert len(exp_4_in_context) == 10\n",
    "assert len(exp_5_in_context) == 10\n",
    "assert len(exp_6_in_context) == 10\n",
    "# Assert length of test\n",
    "assert len(exp_1_test) == 100\n",
    "assert len(exp_2_test) == 100\n",
    "assert len(exp_3_test) == 100\n",
    "assert len(exp_4_test) == 50\n",
    "assert len(exp_5_test) == 50\n",
    "assert len(exp_6_test) == 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use custom rouge function to obtain rouge 3/4 which are not available in huggingface\n",
    "def get_rouge_score(gold, pred):\n",
    "    rouge_scores = ['rouge1', 'rouge2', 'rouge3', 'rouge4', 'rougeL']\n",
    "    scorer = rouge_scorer.RougeScorer(rouge_scores, use_stemmer=True)\n",
    "    scores = scorer.score(gold, pred)\n",
    "    return {k: scores[k].fmeasure * 100 for k in rouge_scores}\n",
    "\n",
    "def compute_custom_metrics(srcs, golds, preds, device):\n",
    "    scores = defaultdict(list)\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "    sari = evaluate.load(\"sari\")\n",
    "    \n",
    "    # For rouge and length go over examples one by one and determine mean\n",
    "    for gold, pred in zip(golds, preds):\n",
    "        for k, v in get_rouge_score(gold, pred).items():\n",
    "            scores[k].append(v)\n",
    "        scores['words'].append(len(pred.split(' ')))\n",
    "    for k, v in scores.items():\n",
    "        scores[k] = np.mean(v)\n",
    "\n",
    "    # This is the default call using model_type=\"roberta-large\"\n",
    "    # This is the same as in the paper \"Generation of Patient After-Visit Summaries to Support Physicians\" (AVS_gen/eval_summarization.py) using the libary SummerTime\n",
    "    scores['bert_score'] = np.mean((bertscore.compute(predictions=preds, references=golds, lang=\"en\", device=device))['f1']) * 100\n",
    "    # BERTScore authors recommend \"microsoft/deberta-large-mnli\" (https://github.com/Tiiiger/bert_score)\n",
    "    scores['bert_score_deberta-large'] = np.mean((bertscore.compute(predictions=preds, references=golds, device=device, model_type=\"microsoft/deberta-large-mnli\"))['f1']) * 100\n",
    "    scores['sari'] = sari.compute(sources=srcs, predictions=preds, references=[[g] for g in golds])['sari']\n",
    "    # scores['sari'] = scores['sari'][0]\n",
    "    # Importing readability for dallc score not working: https://pypi.org/project/py-readability-metrics/    \n",
    "\n",
    "    return {k: round(v, 2) for k, v in scores.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on 1 validation examples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 5.19,\n",
       " 'rouge2': 0.0,\n",
       " 'rouge3': 0.0,\n",
       " 'rouge4': 0.0,\n",
       " 'rougeL': 5.19,\n",
       " 'words': 5.0,\n",
       " 'bert_score': 83.34,\n",
       " 'bert_score_deberta-large': 43.23,\n",
       " 'sari': 50.95}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating prompt\n",
    "\n",
    "# To obtain the valid performance on the 10 validation examples\n",
    "prompt_valid_gpt_predicitions = []\n",
    "prompt_valid_gpt_predicitions.append(\"This is a test prediction.\")\n",
    "prompt_valid_gpt_predicitions.append(\"\")\n",
    "prompt_valid_gpt_predicitions.append(\"\")\n",
    "prompt_valid_gpt_predicitions.append(\"\")\n",
    "prompt_valid_gpt_predicitions.append(\"\")\n",
    "prompt_valid_gpt_predicitions.append(\"\")\n",
    "prompt_valid_gpt_predicitions.append(\"\")\n",
    "prompt_valid_gpt_predicitions.append(\"\")\n",
    "prompt_valid_gpt_predicitions.append(\"\")\n",
    "prompt_valid_gpt_predicitions.append(\"\")\n",
    "\n",
    "srcs = []\n",
    "golds = []\n",
    "preds = []\n",
    "for i, pred in enumerate(prompt_valid_gpt_predicitions):\n",
    "    if pred != \"\":\n",
    "        srcs.append(prompt_valid[i]['text'])\n",
    "        golds.append(prompt_valid[i]['summary'])\n",
    "        preds.append(pred)\n",
    "        \n",
    "print(f\"Evaluate on {len(srcs)} validation examples.\")\n",
    "compute_custom_metrics(srcs, golds, preds, \"cuda\")\n",
    "\n",
    "# Model                                    & R-1 & R-2 & R-3 & R-L & BERTScore & Deberta & SARI & Words \\\\ \\midrule\n",
    "# Llama 2 70B (100 training ex.)           & 43  & 15  & 6   & 25  & 87        & 62      & 44.24 & 125  \\\\"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avs_gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
