{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine Statistics for Annotation Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/s_hegs02/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.metrics.agreement import AnnotationTask\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {'c': 'condition_unsupported', 'p': 'procedure_unsupported', 'm': 'medication_unsupported', 't': 'time_unsupported', 'l': 'location_unsupported', 'n': 'number_unsupported', 'na': 'name_unsupported', 'w': 'word_unsupported', 'o': 'other_unsupported', 'co': 'contradicted_fact', 'i': 'incorrect_fact'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read labelings in BioC format\n",
    "data_path = '/home/s_hegs02/MedTator'\n",
    "data_path = Path(data_path)\n",
    "\n",
    "# Use jsonl files generated from BioC files exported with MedTator\n",
    "dataset_paths = {\n",
    "    # Experiment 1: label mimic summaries\n",
    "    'hallucinations_100_mimic_annotator_1': data_path / '10_label_silver_examples_annotator_1' / 'hallucinations_100_mimic_annotator_1.jsonl',\n",
    "    'hallucinations_100_mimic_annotator_2': data_path / '11_label_silver_examples_annotator_2' / 'hallucinations_100_mimic_annotator_2.jsonl',\n",
    "    'hallucinations_100_mimic_agreed': data_path / '12_agreed_label_silver_examples' / 'hallucinations_100_mimic_agreed.jsonl',\n",
    "    # 'hallucinations_10_valid_mimic_agreed': data_path / '13_agreed_label_silver_validation_examples' / 'hallucinations_10_valid_mimic_agreed.jsonl',\n",
    "    # Experiment 2: label generated summaries\n",
    "    'hallucinations_100_generated_annotator_1': data_path / '20_label_halus_qualitative_annotator_1' / 'hallucinations_100_generated_annotator_1.jsonl',\n",
    "    'hallucinations_100_generated_annotator_2': data_path / '21_label_halus_qualitative_annotator_2' / 'hallucinations_100_generated_annotator_2.jsonl',\n",
    "    'hallucinations_100_generated_agreed': data_path / '22_label_halus_qualitative_agreed' / 'hallucinations_100_generated_agreed.jsonl',\n",
    "}\n",
    "\n",
    "# Read jsonl files\n",
    "def read_jsonl(file_name):\n",
    "    with open(file_name, \"r\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "    \n",
    "# Read jsonl files\n",
    "datasets = {k: read_jsonl(v) for k, v in dataset_paths.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset hallucinations_100_mimic_annotator_1: 100 examples with 239 labels\n",
      "Dataset hallucinations_100_mimic_annotator_2: 100 examples with 282 labels\n",
      "Dataset hallucinations_100_mimic_agreed: 100 examples with 286 labels\n",
      "Dataset hallucinations_100_generated_annotator_1: 100 examples with 123 labels\n",
      "Dataset hallucinations_100_generated_annotator_2: 100 examples with 118 labels\n",
      "Dataset hallucinations_100_generated_agreed: 100 examples with 114 labels\n"
     ]
    }
   ],
   "source": [
    "# Print basic stats\n",
    "\n",
    "def print_stats(name, dataset):\n",
    "    print(f\"Dataset {name}: {len(dataset)} examples with {sum([len(d['labels']) for d in dataset])} labels\")\n",
    "    \n",
    "for name, dataset in datasets.items():\n",
    "    print_stats(name, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       same_entities_same_labels  same_entities_different_labels  \\\n",
      "count                 100.000000                        100.0000   \n",
      "mean                    0.430000                          0.2400   \n",
      "std                     0.781801                          0.6215   \n",
      "min                     0.000000                          0.0000   \n",
      "25%                     0.000000                          0.0000   \n",
      "50%                     0.000000                          0.0000   \n",
      "75%                     1.000000                          0.0000   \n",
      "max                     3.000000                          3.0000   \n",
      "\n",
      "       entities_only_annotator_1  entities_only_annotator_2      doc_id  \\\n",
      "count                 100.000000                 100.000000  100.000000   \n",
      "mean                    0.130000                   0.340000   38.550000   \n",
      "std                     0.366667                   0.589813    7.201817   \n",
      "min                     0.000000                   0.000000   20.000000   \n",
      "25%                     0.000000                   0.000000   33.750000   \n",
      "50%                     0.000000                   0.000000   39.000000   \n",
      "75%                     0.000000                   1.000000   44.250000   \n",
      "max                     2.000000                   3.000000   49.000000   \n",
      "\n",
      "         model_id  perm_model_id  entities_annotator_1  entities_annotator_2  \\\n",
      "count  100.000000     100.000000            100.000000            100.000000   \n",
      "mean     2.000000       2.000000              1.230000              1.180000   \n",
      "std      1.421338       1.421338              1.516608              1.578773   \n",
      "min      0.000000       0.000000              0.000000              0.000000   \n",
      "25%      1.000000       1.000000              0.000000              0.000000   \n",
      "50%      2.000000       2.000000              1.000000              1.000000   \n",
      "75%      3.000000       3.000000              1.250000              2.000000   \n",
      "max      4.000000       4.000000              7.000000              7.000000   \n",
      "\n",
      "       entities_agreement  entities_removed_annotator_1  \\\n",
      "count          100.000000                    100.000000   \n",
      "mean             1.140000                      0.430000   \n",
      "std              1.699792                      0.670519   \n",
      "min              0.000000                      0.000000   \n",
      "25%              0.000000                      0.000000   \n",
      "50%              0.500000                      0.000000   \n",
      "75%              1.250000                      1.000000   \n",
      "max              7.000000                      4.000000   \n",
      "\n",
      "       entities_removed_annotator_2  \n",
      "count                    100.000000  \n",
      "mean                       0.170000  \n",
      "std                        0.403395  \n",
      "min                        0.000000  \n",
      "25%                        0.000000  \n",
      "50%                        0.000000  \n",
      "75%                        0.000000  \n",
      "max                        2.000000  \n"
     ]
    }
   ],
   "source": [
    "# Read statistics collecte during agreement\n",
    "# Labeling 1:\n",
    "# data = pd.read_csv(data_path / '12_agreed_label_silver_examples' / 'agreement_statistics.csv', index_col=0)\n",
    "# \n",
    "# # Add column to dataframe with number of annotations of annotator 1, annotator 2 and agreement\n",
    "# data['entities_annotator_1'] = [len(ex['labels']) for ex in datasets['hallucinations_100_mimic_annotator_1']]\n",
    "# data['entities_annotator_2'] = [len(ex['labels']) for ex in datasets['hallucinations_100_mimic_annotator_2']]\n",
    "# data['entities_agreement'] = [len(ex['labels']) for ex in datasets['hallucinations_100_mimic_agreed']]\n",
    "\n",
    "# Labeling 2:\n",
    "data_permuted = pd.read_csv(data_path / '22_label_halus_qualitative_agreed' / 'agreement_statistics.csv', index_col=0)\n",
    "# Experiment two was ordered into 20 examples per model, the agreement statistics contain all 5 models per summary consecutively\n",
    "# So re-order agreement statistics to match the order of the data\n",
    "data_permuted['doc_id'] = data_permuted.index.str.split('-').str[0].astype(int)\n",
    "data_permuted['model_id'] = data_permuted.index.str.split('-').str[1].astype(int)\n",
    "data_permuted = data_permuted.sort_values(['model_id', 'doc_id'])\n",
    "# Now also undo permutation\n",
    "# 20: [4, 0, 2, 3, 1]\n",
    "hallucination_random_models = {0: [0, 4, 3, 1, 2], 1: [4, 1, 3, 2, 0], 2: [0, 2, 1, 3, 4], 3: [1, 0, 3, 4, 2], 4: [3, 2, 4, 1, 0], 5: [3, 2, 1, 4, 0], 6: [1, 2, 3, 0, 4], 7: [1, 3, 2, 4, 0], 8: [4, 0, 1, 3, 2], 9: [0, 3, 2, 4, 1], 10: [0, 4, 3, 2, 1], 11: [1, 0, 4, 3, 2], 12: [2, 4, 1, 0, 3], 13: [3, 1, 0, 4, 2], 14: [4, 2, 0, 1, 3], 15: [0, 2, 4, 3, 1], 16: [1, 4, 2, 3, 0], 17: [2, 3, 1, 0, 4], 18: [4, 0, 3, 2, 1], 19: [0, 3, 1, 2, 4], 20: [4, 0, 2, 3, 1], 21: [0, 4, 2, 1, 3], 22: [0, 2, 4, 3, 1], 23: [1, 0, 3, 4, 2], 24: [3, 1, 0, 4, 2], 25: [2, 0, 3, 4, 1], 26: [4, 3, 0, 1, 2], 27: [3, 4, 2, 1, 0], 28: [4, 2, 3, 1, 0], 29: [4, 1, 3, 0, 2], 30: [2, 3, 0, 1, 4], 31: [4, 2, 0, 3, 1], 32: [3, 0, 2, 1, 4], 33: [2, 3, 4, 1, 0], 34: [4, 1, 3, 2, 0], 35: [0, 4, 1, 3, 2], 36: [4, 1, 3, 0, 2], 37: [3, 1, 0, 4, 2], 38: [3, 2, 4, 1, 0], 39: [1, 0, 3, 4, 2], 40: [4, 3, 0, 1, 2], 41: [2, 3, 4, 0, 1], 42: [2, 4, 3, 1, 0], 43: [4, 1, 2, 0, 3], 44: [0, 4, 3, 1, 2], 45: [3, 2, 0, 1, 4], 46: [2, 4, 0, 3, 1], 47: [2, 1, 0, 4, 3], 48: [4, 2, 3, 1, 0], 49: [3, 1, 4, 2, 0]}\n",
    "data = data_permuted.copy()\n",
    "replaced_columns = ['same_entities_same_labels', 'same_entities_different_labels', 'entities_only_annotator_1', 'entities_only_annotator_2']\n",
    "data[replaced_columns] = pd.DataFrame([[None, None, None, None]], index=data.index)\n",
    "# Iterate over all rows and copy values from data_permuted to data\n",
    "for i, row in data.iterrows():\n",
    "    # Set all for columns in data\n",
    "    doc_id = row['doc_id']\n",
    "    perm_model_id = hallucination_random_models[row['doc_id']].index(row['model_id'])\n",
    "    # print(doc_id, perm_model_id)\n",
    "    # Get values from data_permuted with doc_id and perm_model_id\n",
    "    for col in replaced_columns:\n",
    "        data.at[i, col] = data_permuted.loc[(data_permuted['doc_id'] == doc_id) & (data_permuted['model_id'] == perm_model_id), col].values[0]\n",
    "    data.at[i, 'perm_model_id'] = perm_model_id\n",
    "    \n",
    "# Add column to dataframe with number of annotations of annotator 1, annotator 2 and agreement\n",
    "data['entities_annotator_1'] = [len(ex['labels']) for ex in datasets['hallucinations_100_generated_annotator_1']]\n",
    "data['entities_annotator_2'] = [len(ex['labels']) for ex in datasets['hallucinations_100_generated_annotator_2']]\n",
    "data['entities_agreement'] = [len(ex['labels']) for ex in datasets['hallucinations_100_generated_agreed']]\n",
    "\n",
    "\n",
    "\n",
    "# Iterate over all rows an add counts for number of annotations removed per annotator\n",
    "for i, row in data.iterrows():\n",
    "    # Add counts to dataframe\n",
    "    data.at[i, 'entities_removed_annotator_1'] = data.at[i, 'entities_annotator_1'] - data.at[i, 'same_entities_same_labels'] - data.at[i, 'same_entities_different_labels'] - data.at[i, 'entities_only_annotator_1']\n",
    "    data.at[i, 'entities_removed_annotator_2'] = data.at[i, 'entities_annotator_2'] - data.at[i, 'same_entities_same_labels'] - data.at[i, 'same_entities_different_labels'] - data.at[i, 'entities_only_annotator_2']\n",
    "\n",
    "# Convert complete row to int\n",
    "data = data.astype(int)\n",
    "    \n",
    "for i, row in data.iterrows():\n",
    "    # Check counts\n",
    "    assert data.at[i, 'entities_removed_annotator_1'] >= 0,f\"wrong count annotator 1 {i} {row}\"\n",
    "    assert data.at[i, 'entities_removed_annotator_2'] >= 0,f\"wrong count annotator 2 {i} {row}\"\n",
    "    assert data.at[i, 'entities_annotator_1'] == data.at[i, 'entities_removed_annotator_1'] + data.at[i, 'same_entities_same_labels'] + data.at[i, 'same_entities_different_labels'] + data.at[i, 'entities_only_annotator_1'],f\"wrong count annotator 1 {i} {row}\"\n",
    "    assert data.at[i, 'entities_annotator_2'] == data.at[i, 'entities_removed_annotator_2'] + data.at[i, 'same_entities_same_labels'] + data.at[i, 'same_entities_different_labels'] + data.at[i, 'entities_only_annotator_2'],f\"wrong count annotator 2 {i} {row}\"\n",
    "    assert data.at[i, 'entities_agreement'] == data.at[i, 'same_entities_same_labels'] + data.at[i, 'same_entities_different_labels'] + data.at[i, 'entities_only_annotator_1'] + data.at[i, 'entities_only_annotator_2'],f\"wrong count agreement {i} {row}\"\n",
    "\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lc}\n",
      "\\toprule\n",
      " & Mean (std) \\\\\n",
      "\\midrule\n",
      "Annotations annotator 1 & 1.23 (1.52) \\\\\n",
      "Removed in agreement 1 & 0.43 (0.67) \\\\\n",
      "Annotations annotator 2 & 1.18 (1.58) \\\\\n",
      "Removed in agreement 2 & 0.17 (0.40) \\\\\n",
      "Annotations agreement & 1.14 (1.70) \\\\\n",
      "Both annotators, same label & 0.43 (0.78) \\\\\n",
      "Both annotators, different label & 0.24 (0.62) \\\\\n",
      "Only annotator 1 & 0.13 (0.37) \\\\\n",
      "Only annotator 2 & 0.34 (0.59) \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Format statistics as latex table\n",
    "statistics_dict = {\n",
    "    'Annotations annotator 1': f\"{data['entities_annotator_1'].mean():.2f} ({data['entities_annotator_1'].std():.2f})\",\n",
    "    'Removed in agreement 1': f\"{data['entities_removed_annotator_1'].mean():.2f} ({data['entities_removed_annotator_1'].std():.2f})\",\n",
    "    'Annotations annotator 2': f\"{data['entities_annotator_2'].mean():.2f} ({data['entities_annotator_2'].std():.2f})\",\n",
    "    'Removed in agreement 2': f\"{data['entities_removed_annotator_2'].mean():.2f} ({data['entities_removed_annotator_2'].std():.2f})\",\n",
    "    'Annotations agreement': f\"{data['entities_agreement'].mean():.2f} ({data['entities_agreement'].std():.2f})\",\n",
    "    'Both annotators, same label': f\"{data['same_entities_same_labels'].mean():.2f} ({data['same_entities_same_labels'].std():.2f})\",\n",
    "    'Both annotators, different label': f\"{data['same_entities_different_labels'].mean():.2f} ({data['same_entities_different_labels'].std():.2f})\",\n",
    "    'Only annotator 1': f\"{data['entities_only_annotator_1'].mean():.2f} ({data['entities_only_annotator_1'].std():.2f})\",\n",
    "    'Only annotator 2': f\"{data['entities_only_annotator_2'].mean():.2f} ({data['entities_only_annotator_2'].std():.2f})\",\n",
    "}\n",
    "\n",
    "# Dict to dataframe\n",
    "statistics_df = pd.DataFrame.from_dict(statistics_dict, orient='index', columns=['Mean (std)'])\n",
    "# Print as latex, convert second column to centered\n",
    "print(statistics_df.to_latex(column_format='lc', escape=False))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lccc}\n",
      "\\toprule\n",
      " & A 1 & A 2 & Agree \\\\\n",
      "\\midrule\n",
      "condition unsupported & 25 & 27 & 27 \\\\\n",
      "procedure unsupported & 12 & 3 & 4 \\\\\n",
      "medication unsupported & 13 & 10 & 10 \\\\\n",
      "time unsupported & 3 & 3 & 2 \\\\\n",
      "location unsupported & 13 & 4 & 12 \\\\\n",
      "number unsupported & 0 & 4 & 3 \\\\\n",
      "name unsupported & 5 & 6 & 5 \\\\\n",
      "word unsupported & 49 & 48 & 44 \\\\\n",
      "other unsupported & 1 & 5 & 0 \\\\\n",
      "contradicted fact & 2 & 8 & 7 \\\\\n",
      "incorrect fact & 0 & 0 & 0 \\\\\n",
      "Total & 123 & 118 & 114 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get table for error types\n",
    "def get_label_count(dataset):\n",
    "    label_counts = {l: 0 for l in labels.values()}\n",
    "    for ex in dataset:\n",
    "        for label in ex[\"labels\"]:\n",
    "            label_counts[label[\"label\"]] += 1\n",
    "    return label_counts\n",
    "\n",
    "# Labeling 1\n",
    "# label_counts_1 = get_label_count(datasets['hallucinations_100_mimic_annotator_1'])\n",
    "# label_counts_2 = get_label_count(datasets['hallucinations_100_mimic_annotator_2'])\n",
    "# label_counts_agreement = get_label_count(datasets['hallucinations_100_mimic_agreed'])\n",
    "\n",
    "# Labeling 2\n",
    "label_counts_1 = get_label_count(datasets['hallucinations_100_generated_annotator_1'])\n",
    "label_counts_2 = get_label_count(datasets['hallucinations_100_generated_annotator_2'])\n",
    "label_counts_agreement = get_label_count(datasets['hallucinations_100_generated_agreed'])\n",
    "        \n",
    "# Convert to one dataframe with label names in first column\n",
    "label_counts_1_df = pd.DataFrame.from_dict(label_counts_1, orient='index', columns=['A 1'])\n",
    "label_counts_2_df = pd.DataFrame.from_dict(label_counts_2, orient='index', columns=['A 2'])\n",
    "label_counts_agreement_df = pd.DataFrame.from_dict(label_counts_agreement, orient='index', columns=['Agree'])\n",
    "# concat\n",
    "label_counts_df = pd.concat([label_counts_1_df, label_counts_2_df, label_counts_agreement_df], axis=1)\n",
    "\n",
    "# Add row total\n",
    "label_counts_df.loc['Total'] = label_counts_df.sum(axis=0)\n",
    "\n",
    "# Replace underscore in label names with space\n",
    "label_counts_df.index = label_counts_df.index.str.replace('_', ' ')\n",
    "\n",
    "# Output as latex table\n",
    "print(label_counts_df.to_latex(column_format='lccc', escape=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Krippendorf's alpha: 0.826\n"
     ]
    }
   ],
   "source": [
    "# Determine inter-annotator agreement for hallucination annotations\n",
    "# Determin krippendorf alpha based on annotations counts\n",
    "\n",
    "annotation_triples = []\n",
    "for i, row in data.iterrows():\n",
    "    annotation_triples.append(('coder_1', i, row['entities_annotator_1']))\n",
    "    annotation_triples.append(('coder_2', i, row['entities_annotator_2']))\n",
    "\n",
    "t = AnnotationTask(annotation_triples, distance=interval_distance)\n",
    "print(f\"Krippendorf's alpha: {t.alpha():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word level F1 (class aware): 0.271\n",
      "Word level F1 (class agnostic): 0.440\n"
     ]
    }
   ],
   "source": [
    "# Determine word-level F1 overlap\n",
    "\n",
    "# Define tokenizer\n",
    "tokenizer = lambda x: wordpunct_tokenize(x)\n",
    "\n",
    "# Taken from hallucination evaluation script\n",
    "def character_labels_to_word_labels(text, labels):\n",
    "    # Convert character level labels to word level labels\n",
    "    new_labels = []\n",
    "    for label in labels:\n",
    "        new_label = {'label': label['label']}\n",
    "        new_label['start'] = len(tokenizer(text[:label['start']]))\n",
    "        new_label['end'] = new_label['start'] + len(tokenizer(label['text']))\n",
    "        new_label['length'] = new_label['end'] - new_label['start']\n",
    "        # Copy over old text because not tokenized version, but check it contains same text without whitespace\n",
    "        new_label['text'] = label['text']\n",
    "        # print(label['start'], tokenizer(text)[new_label['start']:new_label['end']], 'vs', label['text'])\n",
    "        assert ''.join(tokenizer(text)[new_label['start']:new_label['end']]) == label['text'].replace(' ', '')\n",
    "        new_labels.append(new_label)\n",
    "    return new_labels\n",
    "\n",
    "def change_labels_to_single_words(labels):\n",
    "    new_labels = []\n",
    "    for label in labels:\n",
    "        for i in range(label['length']):\n",
    "            new_label = {'label': label['label']}\n",
    "            new_label['start'] = label['start'] + i\n",
    "            new_label['end'] = label['start'] + i + 1\n",
    "            new_label['length'] = 1\n",
    "            new_label['text'] = tokenizer(label['text'])[i]\n",
    "            new_labels.append(new_label)\n",
    "    return new_labels\n",
    "\n",
    "# Convert all labeling to word level\n",
    "# Labeling 1\n",
    "# annotator_1_word_labels = datasets['hallucinations_100_mimic_annotator_1']\n",
    "# annotator_2_word_labels = datasets['hallucinations_100_mimic_annotator_2']\n",
    "# # Manually fix non-word aligned labels - only first character of word was missed - does not affect evaluation\n",
    "# annotator_2_word_labels[49]['labels'][1] = {'start': 209, 'end': 257, 'length': 48, 'label': 'word_unsupported', 'text': 'The reports of these tests were provided to you.'}\n",
    "# annotator_2_word_labels[49]['labels'][6] = {'start': 577, 'end': 670, 'length': 93, 'label': 'word_unsupported', 'text': 'Your primary care doctor, ___ your gastroenterologist, Dr. ___ aware of your hospitalization.'}\n",
    "# annotator_2_word_labels[87]['labels'][0] = {'start': 432, 'end': 513, 'length': 81, 'label': 'other_unsupported', 'text': 'may be a manifestation of stress and may be related to your psychiatric illnesses'}\n",
    "\n",
    "# Labeling 2\n",
    "annotator_1_word_labels = datasets['hallucinations_100_generated_annotator_1']\n",
    "annotator_2_word_labels = datasets['hallucinations_100_generated_annotator_2']\n",
    "annotator_2_word_labels[24]['labels'][3] = {'start': 635, 'end': 651, 'length': 16, 'label': 'condition_unsupported', 'text': 'which was normal'}\n",
    "\n",
    "# Convert to word labels\n",
    "annotator_1_word_labels = [character_labels_to_word_labels(ex['summary'], ex['labels']) for ex in annotator_1_word_labels]\n",
    "annotator_2_word_labels = [character_labels_to_word_labels(ex['summary'], ex['labels']) for ex in annotator_2_word_labels]                                               \n",
    "# Convert to single word labels\n",
    "annotator_1_word_labels_single = [change_labels_to_single_words(labels) for labels in annotator_1_word_labels]\n",
    "annotator_2_word_labels_single = [change_labels_to_single_words(labels) for labels in annotator_2_word_labels]\n",
    "\n",
    "def determine_word_level_f1(in_annotator_1, in_annotator_2):\n",
    "    labels_1_in_labels_2 = 0\n",
    "    total_labels_1 = 0\n",
    "    labels_2_in_labels_1 = 0\n",
    "    total_labels_2 = 0\n",
    "    for i in range(len(in_annotator_1)):\n",
    "        doc_labels_1 = in_annotator_1[i]\n",
    "        doc_labels_2 = in_annotator_2[i]\n",
    "        labels_1_in_labels_2 += len([l for l in doc_labels_1 if l in doc_labels_2])\n",
    "        total_labels_1 += len(doc_labels_1)\n",
    "        labels_2_in_labels_1 += len([l for l in doc_labels_2 if l in doc_labels_1])\n",
    "        total_labels_2 += len(doc_labels_2)\n",
    "    precision = labels_1_in_labels_2 / total_labels_1\n",
    "    recall = labels_2_in_labels_1 / total_labels_2\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    return f1, precision, recall\n",
    "\n",
    "# Class aware\n",
    "f1, precision, recall = determine_word_level_f1(annotator_1_word_labels_single, annotator_2_word_labels_single)\n",
    "print(f\"Word level F1 (class aware): {f1:.3f}\")\n",
    "\n",
    "# Class agnostic\n",
    "# Only keep start positions of labels as these are unique identifiers\n",
    "annotator_1_word_labels_starts = [[label['start'] for label in labels] for labels in annotator_1_word_labels_single]\n",
    "annotator_2_word_labels_starts = [[label['start'] for label in labels] for labels in annotator_2_word_labels_single]\n",
    "f1, precision, recall = determine_word_level_f1(annotator_1_word_labels_starts, annotator_2_word_labels_starts)\n",
    "print(f\"Word level F1 (class agnostic): {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lccc}\n",
      "\\toprule\n",
      " & llama_70b_original & llama_70b_cleaned & gpt4_orig & gpt4_cleaned & gpt4_zero_shot \\\\\n",
      "\\midrule\n",
      "condition unsupported & 16 & 7 & 2 & 1 & 1 \\\\\n",
      "procedure unsupported & 2 & 2 & 0 & 0 & 0 \\\\\n",
      "medication unsupported & 9 & 1 & 0 & 0 & 0 \\\\\n",
      "time unsupported & 1 & 1 & 0 & 0 & 0 \\\\\n",
      "location unsupported & 4 & 5 & 1 & 2 & 0 \\\\\n",
      "number unsupported & 3 & 0 & 0 & 0 & 0 \\\\\n",
      "name unsupported & 3 & 1 & 1 & 0 & 0 \\\\\n",
      "word unsupported & 11 & 10 & 10 & 5 & 8 \\\\\n",
      "other unsupported & 0 & 0 & 0 & 0 & 0 \\\\\n",
      "contradicted fact & 3 & 4 & 0 & 0 & 0 \\\\\n",
      "incorrect fact & 0 & 0 & 0 & 0 & 0 \\\\\n",
      "Total & 52 & 31 & 14 & 8 & 9 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Additional analysis for labeling2 - create label wise hallucination counts for each model\n",
    "# Check for row groups of 20 (0-19, 20-39, ...) the mean and std of the number of hallucinations (column: entities_agreement)\n",
    "models = [0,1,2,3,4]\n",
    "model_names = ['llama_70b_original', 'llama_70b_cleaned', 'gpt4_zero_shot', 'gpt4_orig', 'gpt4_cleaned']\n",
    "summaries_per_model = 20\n",
    "label_counts_df = []\n",
    "for model in models:\n",
    "    range_idx = (model * summaries_per_model, (model + 1) * summaries_per_model)\n",
    "    label_counts_agreement = get_label_count(datasets['hallucinations_100_generated_agreed'][range_idx[0]:range_idx[1]])\n",
    "    label_counts_agreement_df = pd.DataFrame.from_dict(label_counts_agreement, orient='index', columns=[model_names[model]])\n",
    "    label_counts_df.append(label_counts_agreement_df)\n",
    "    \n",
    "# Concat\n",
    "label_counts_df = pd.concat(label_counts_df, axis=1)\n",
    "\n",
    "# Add row total\n",
    "label_counts_df.loc['Total'] = label_counts_df.sum(axis=0)\n",
    "\n",
    "# Replace underscore in label names with space\n",
    "label_counts_df.index = label_counts_df.index.str.replace('_', ' ')\n",
    "\n",
    "# Reorder columns\n",
    "label_counts_df = label_counts_df[['llama_70b_original', 'llama_70b_cleaned', 'gpt4_orig', 'gpt4_cleaned', 'gpt4_zero_shot']]\n",
    "\n",
    "# Output as latex table\n",
    "print(label_counts_df.to_latex(column_format='lccc', escape=False))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model llama_70b_original: $2.60$ ($2.39$)\n",
      "Model llama_70b_cleaned: $1.55$ ($1.99$)\n",
      "Model gpt4_zero_shot: $0.45$ ($0.60$)\n",
      "Model gpt4_orig: $0.70$ ($0.86$)\n",
      "Model gpt4_cleaned: $0.40$ ($0.75$)\n"
     ]
    }
   ],
   "source": [
    "# Additional analysis for labeling2 - how many hallucinations for each model\n",
    "# Check for row groups of 20 (0-19, 20-39, ...) the mean and std of the number of hallucinations (column: entities_agreement)\n",
    "models = [0,1,2,3,4]\n",
    "model_names = ['llama_70b_original', 'llama_70b_cleaned', 'gpt4_zero_shot', 'gpt4_orig', 'gpt4_cleaned']\n",
    "summaries_per_model = 20\n",
    "for model in models:\n",
    "    range_idx = (model * summaries_per_model, (model + 1) * summaries_per_model)\n",
    "    # Print index of summaries for each model\n",
    "    # print(range_idx)\n",
    "    # print(f\"Model {model_names[model]}: {data.iloc[range_idx[0]:range_idx[1]]['doc_id'].values}\")\n",
    "    # print(f\"Model {model_names[model]}: {data.iloc[range_idx[0]:range_idx[1]]['model_id'].values}\")\n",
    "    print(f\"Model {model_names[model]}: ${data.iloc[range_idx[0]:range_idx[1]]['entities_agreement'].mean():.2f}$ (${data.iloc[range_idx[0]:range_idx[1]]['entities_agreement'].std():.2f}$)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model llama_70b_original: [1]\n",
      "Model llama_70b_cleaned: [7]\n",
      "Model gpt4_zero_shot: [2]\n",
      "Model gpt4_orig: [2]\n",
      "Model gpt4_cleaned: [0]\n",
      "\n",
      "Model llama_70b_original: [6]\n",
      "Model llama_70b_cleaned: [3]\n",
      "Model gpt4_zero_shot: [0]\n",
      "Model gpt4_orig: [1]\n",
      "Model gpt4_cleaned: [1]\n"
     ]
    }
   ],
   "source": [
    "# Additional analysis for labeling2 - sanity check for model output\n",
    "# Print num labels for example 33 and 34 because there every model has different number of hallucinations\n",
    "def print_num_labels(doc_id):\n",
    "    for model in models:\n",
    "        print(f\"Model {model_names[model]}: {data.loc[(data['doc_id'] == doc_id) & (data['model_id'] == model)]['entities_agreement'].values}\")\n",
    "\n",
    "print_num_labels(33)\n",
    "print()\n",
    "print_num_labels(34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model llama_70b_original: $97.90$ ($36.73$)\n",
      "Model llama_70b_cleaned: $96.20$ ($31.82$)\n",
      "Model gpt4_zero_shot: $165.05$ ($22.75$)\n",
      "Model gpt4_orig: $151.10$ ($19.42$)\n",
      "Model gpt4_cleaned: $158.80$ ($23.27$)\n"
     ]
    }
   ],
   "source": [
    "# Additional analysis for labeling2 - get number of words per summary\n",
    "def get_num_words(text):\n",
    "    # use this simplistic approach since same was used for performance table\n",
    "    return len(text.split())\n",
    "\n",
    "dataset_words = datasets['hallucinations_100_generated_agreed']\n",
    "models = [0,1,2,3,4]\n",
    "model_names = ['llama_70b_original', 'llama_70b_cleaned', 'gpt4_zero_shot', 'gpt4_orig', 'gpt4_cleaned']\n",
    "summaries_per_model = 20\n",
    "for model in models:\n",
    "    range_idx = (model * summaries_per_model, (model + 1) * summaries_per_model)\n",
    "    # Print index of summaries for each model\n",
    "    num_words = np.array([get_num_words(ex['summary']) for ex in dataset_words[range_idx[0]:range_idx[1]]])\n",
    "    # print(range_idx)\n",
    "    assert len(num_words) == summaries_per_model\n",
    "    # Mean and std over all summaries\n",
    "    print(f\"Model {model_names[model]}: ${np.mean(num_words):.2f}$ (${np.std(num_words):.2f}$)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotator 1:\n",
      "  Number of documents: 20\n",
      "Annotator 2:\n",
      "  Number of documents: 20\n",
      "\\begin{tabular}{lcc}\n",
      "\\toprule\n",
      "Model & KP & MJ \\\\\n",
      "\\midrule\n",
      "llama_70b_original & $3.77$ ($1.33$) & $1.05$ ($0.84$) \\\\\n",
      "llama_70b_cleaned & $3.73$ ($1.45$) & $1.68$ ($1.23$) \\\\\n",
      "gpt4_zero_shot & $0.82$ ($0.61$) & $0.70$ ($1.03$) \\\\\n",
      "gpt4_orig & $0.93$ ($0.80$) & $1.07$ ($0.99$) \\\\\n",
      "gpt4_cleaned & $0.97$ ($0.80$) & $1.25$ ($1.18$) \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Additional analysis for labeling2 - determine mean and std of key facts, medical jargon, and words\n",
    "\n",
    "key_jargon_paths = {\n",
    "    # Experiment 3: label key facts and medical jargon in the hallucination reduction task\n",
    "    'key_jargon_annotator_1': data_path / '30_label_key_jargon_annotator_1' / 'key_jargon_annotator_1.xlsx',\n",
    "    'key_jargon_annotator_2': data_path / '31_label_key_jargon_annotator_2' / 'key_jargon_annotator_2.xlsx',\n",
    "}\n",
    "\n",
    "# Rating columns\n",
    "num_models = 5\n",
    "columns = [[f\"KP {i}\", f\"MJ {i}\"] for i in range(0, num_models)]\n",
    "columns = [item for sublist in columns for item in sublist] + ['Total KP']\n",
    "\n",
    "def prepare_df(path):\n",
    "    df = pd.read_excel(path, sheet_name=0, header=11)\n",
    "    # Remove columns starting with Unnamed\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "    df.set_index('id', inplace=True)\n",
    "    assert df.values.min() >= 0 and df.values.max() <= 15\n",
    "    assert df.isnull().values.any() == False\n",
    "    assert set(columns) == set(df.columns)\n",
    "    print(f\"  Number of documents: {len(df)}\")\n",
    "    return df\n",
    "\n",
    "hallucination_random_models = {0: [0, 4, 3, 1, 2], 1: [4, 1, 3, 2, 0], 2: [0, 2, 1, 3, 4], 3: [1, 0, 3, 4, 2], 4: [3, 2, 4, 1, 0], 5: [3, 2, 1, 4, 0], 6: [1, 2, 3, 0, 4], 7: [1, 3, 2, 4, 0], 8: [4, 0, 1, 3, 2], 9: [0, 3, 2, 4, 1], 10: [0, 4, 3, 2, 1], 11: [1, 0, 4, 3, 2], 12: [2, 4, 1, 0, 3], 13: [3, 1, 0, 4, 2], 14: [4, 2, 0, 1, 3], 15: [0, 2, 4, 3, 1], 16: [1, 4, 2, 3, 0], 17: [2, 3, 1, 0, 4], 18: [4, 0, 3, 2, 1], 19: [0, 3, 1, 2, 4], 20: [4, 0, 2, 3, 1], 21: [0, 4, 2, 1, 3], 22: [0, 2, 4, 3, 1], 23: [1, 0, 3, 4, 2], 24: [3, 1, 0, 4, 2], 25: [2, 0, 3, 4, 1], 26: [4, 3, 0, 1, 2], 27: [3, 4, 2, 1, 0], 28: [4, 2, 3, 1, 0], 29: [4, 1, 3, 0, 2], 30: [2, 3, 0, 1, 4], 31: [4, 2, 0, 3, 1], 32: [3, 0, 2, 1, 4], 33: [2, 3, 4, 1, 0], 34: [4, 1, 3, 2, 0], 35: [0, 4, 1, 3, 2], 36: [4, 1, 3, 0, 2], 37: [3, 1, 0, 4, 2], 38: [3, 2, 4, 1, 0], 39: [1, 0, 3, 4, 2], 40: [4, 3, 0, 1, 2], 41: [2, 3, 4, 0, 1], 42: [2, 4, 3, 1, 0], 43: [4, 1, 2, 0, 3], 44: [0, 4, 3, 1, 2], 45: [3, 2, 0, 1, 4], 46: [2, 4, 0, 3, 1], 47: [2, 1, 0, 4, 3], 48: [4, 2, 3, 1, 0], 49: [3, 1, 4, 2, 0]}\n",
    "\n",
    "def undo_randomization(df):\n",
    "    # Copy dataframe\n",
    "    df_unrandomized = df.copy()\n",
    "    df_unrandomized[:] = -1\n",
    "    # For each row use entry with id in qualitative_random_models amd reorder \"Rel {i}\", \"Con {i}\", \"Flu {i}\", \"Coh {i}\" based on the randomization\n",
    "    for id, row in df.iterrows():\n",
    "        df_unrandomized.loc[id, 'Total KP'] = row['Total KP']\n",
    "        for i in range(0, num_models):\n",
    "            df_unrandomized.loc[id, f\"KP {i}\"] = row[f\"KP {hallucination_random_models[id].index(i)}\"]\n",
    "            df_unrandomized.loc[id, f\"MJ {i}\"] = row[f\"MJ {hallucination_random_models[id].index(i)}\"]\n",
    "    return df_unrandomized\n",
    "\n",
    "print(\"Annotator 1:\")\n",
    "rand_key_jargon_annotator_1 = prepare_df(key_jargon_paths['key_jargon_annotator_1'])\n",
    "print(\"Annotator 2:\")\n",
    "rand_key_jargon_annotator_2 = prepare_df(key_jargon_paths['key_jargon_annotator_2'])\n",
    "\n",
    "key_rand_annotator_1 = undo_randomization(rand_key_jargon_annotator_1)\n",
    "key_rand_annotator_2 = undo_randomization(rand_key_jargon_annotator_2)\n",
    "\n",
    "# First take average over both dataframes then calculate mean and std ant output as latex table\n",
    "# This makes more sense to interpet the SD per example\n",
    "key_rand_mean = pd.concat([key_rand_annotator_1, key_rand_annotator_2]).groupby(level=0).mean()\n",
    "# Assert that same columns and still 20 rows\n",
    "assert set(columns) == set(key_rand_mean.columns) and len(key_rand_mean) == 20\n",
    "# print(key_rand_mean)\n",
    "\n",
    "models = ['llama_70b_original', 'llama_70b_cleaned', 'gpt4_zero_shot', 'gpt4_orig', 'gpt4_cleaned']\n",
    "\n",
    "latex_results = []\n",
    "for model in range(0, num_models):\n",
    "    row = []\n",
    "    for metric in [\"KP\", \"MJ\"]:\n",
    "        key = f\"{metric} {model}\"\n",
    "        # With SD\n",
    "        row.append(f\"${key_rand_mean[key].mean():.2f}$ (${key_rand_mean[key].std():.2f}$)\")\n",
    "        # row.append(f\"${key_rand_mean[key].mean():.2f}$\")\n",
    "    latex_results.append([models[model]] + row)\n",
    "    \n",
    "latex_results = pd.DataFrame(latex_results, columns=[\"Model\", \"KP\", \"MJ\"])\n",
    "print(latex_results.to_latex(index=False, escape=False, column_format=\"lcc\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avs_gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
