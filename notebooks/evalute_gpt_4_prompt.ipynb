{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate GPT 4 prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import evaluate\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for prompt 1 - 1 IC-example, prompt 1 - 3 IC-examples, prompt 1 - 5 IC-examples, prompt 2 - 1 IC-example, prompt 2 - 3 IC-examples, prompt 2 - 5 IC-examples, prompt 3 - 1 IC-example, prompt 3 - 3 IC-examples, prompt 3 - 5 IC-examples\n",
    "prefix = Path('/home/s_hegs02/patient_summaries_with_llms/gpt-4/prompt_tuning/')\n",
    "files_paths = [\n",
    "    # 'gpt-4_exp4_results_prompt1_1shot.jsonl',\n",
    "    # 'gpt-4_exp4_results_prompt1_3shot.jsonl',\n",
    "    # 'gpt-4_exp4_results_prompt1_5shot.jsonl',\n",
    "    # 'gpt-4_exp4_results_prompt2_1shot.jsonl',\n",
    "    # 'gpt-4_exp4_results_prompt2_3shot.jsonl',\n",
    "    # 'gpt-4_exp4_results_prompt2_5shot.jsonl',\n",
    "    # # Missing\n",
    "    # 'gpt-4_exp4_results_prompt3_1shot.jsonl',\n",
    "    # 'gpt-4_exp4_results_prompt3_3shot.jsonl',\n",
    "    # 'gpt-4_exp4_results_prompt3_5shot.jsonl',\n",
    "    'gpt-4_exp4_results_prompt3_0shot.jsonl',\n",
    "    'gpt-4_exp4_results_prompt3_5shot.jsonl',\n",
    "]\n",
    "\n",
    "# Read jsonl files\n",
    "def read_jsonl(file_name):\n",
    "    with open(file_name, \"r\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "    \n",
    "files = [read_jsonl(prefix / file_path) for file_path in files_paths]\n",
    "\n",
    "test_data_file = \"/home/s_hegs02/patient_summaries_with_llms/gpt-4/summarization_data/exp_4_test.json\"\n",
    "test_data = read_jsonl(test_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use custom rouge function to obtain rouge 3/4 which are not available in huggingface\n",
    "def get_rouge_score(gold, pred):\n",
    "    rouge_scores = ['rouge1', 'rouge2', 'rouge3', 'rouge4', 'rougeL']\n",
    "    scorer = rouge_scorer.RougeScorer(rouge_scores, use_stemmer=True)\n",
    "    scores = scorer.score(gold, pred)\n",
    "    return {k: scores[k].fmeasure * 100 for k in rouge_scores}\n",
    "\n",
    "def compute_custom_metrics(srcs, golds, preds, device):\n",
    "    scores = defaultdict(list)\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "    sari = evaluate.load(\"sari\")\n",
    "    \n",
    "    # For rouge and length go over examples one by one and determine mean\n",
    "    for gold, pred in zip(golds, preds):\n",
    "        for k, v in get_rouge_score(gold, pred).items():\n",
    "            scores[k].append(v)\n",
    "        scores['words'].append(len(pred.split(' ')))\n",
    "    for k, v in scores.items():\n",
    "        scores[k] = np.mean(v)\n",
    "\n",
    "    # This is the default call using model_type=\"roberta-large\"\n",
    "    # This is the same as in the paper \"Generation of Patient After-Visit Summaries to Support Physicians\" (AVS_gen/eval_summarization.py) using the libary SummerTime\n",
    "    scores['bert_score'] = np.mean((bertscore.compute(predictions=preds, references=golds, lang=\"en\", device=device))['f1']) * 100\n",
    "    # BERTScore authors recommend \"microsoft/deberta-large-mnli\" (https://github.com/Tiiiger/bert_score)\n",
    "    scores['bert_score_deberta-large'] = np.mean((bertscore.compute(predictions=preds, references=golds, device=device, model_type=\"microsoft/deberta-large-mnli\"))['f1']) * 100\n",
    "    scores['sari'] = sari.compute(sources=srcs, predictions=preds, references=[[g] for g in golds])['sari']\n",
    "    # scores['sari'] = scores['sari'][0]\n",
    "    # Importing readability for dallc score not working: https://pypi.org/project/py-readability-metrics/    \n",
    "\n",
    "    return scores\n",
    "\n",
    "def get_metrics_as_latex(metrics):\n",
    "    # Print latex table row\n",
    "    order = ['rouge1', 'rouge2', 'rouge3', 'rouge4', 'rougeL', 'bert_score', 'bert_score_deberta-large', 'sari', 'words']\n",
    "    return ' & '.join([f'${metrics[k]:.2f}$' for k in order])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print performance\n",
    "srcs = [e[\"text\"] for e in test_data][:len(files[0])]\n",
    "golds = [e[\"summary\"] for e in test_data][:len(files[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "$42.50$ & $11.95$ & $4.37$ & $2.09$ & $21.49$ & $86.30$ & $61.36$ & $45.70$ & $214.40$\n",
      "$41.99$ & $12.83$ & $5.22$ & $2.26$ & $22.67$ & $86.95$ & $62.35$ & $43.55$ & $138.70$\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i , f in enumerate(files):\n",
    "    preds = [e[\"summary\"] for e in f]\n",
    "    metrics = compute_custom_metrics(srcs, golds, preds, \"cuda\")\n",
    "    metrics = {k: round(v, 2) for k, v in metrics.items()}\n",
    "    results.append(get_metrics_as_latex(metrics))\n",
    "    print()\n",
    "    \n",
    "print('\\n'.join(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print examples along with summaries\n",
    "\n",
    "for i in range(0, len(files[0])):\n",
    "    print(f\"Example {i+1}\")\n",
    "    print(f\"Source: {srcs[i]}\\n\")\n",
    "    print(f\"Gold: {golds[i]}\\n\")\n",
    "    for j, f in enumerate(files):\n",
    "        print(f\"Summary {j+1}: {' '.join(f[i]['summary'].split())}\\n\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avs_gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
