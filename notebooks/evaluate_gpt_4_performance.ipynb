{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import evaluate\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use custom rouge function to obtain rouge 3/4 which are not available in huggingface\n",
    "def get_rouge_score(gold, pred):\n",
    "    rouge_scores = ['rouge1', 'rouge2', 'rouge3', 'rouge4', 'rougeL']\n",
    "    scorer = rouge_scorer.RougeScorer(rouge_scores, use_stemmer=True)\n",
    "    scores = scorer.score(gold, pred)\n",
    "    return {k: scores[k].fmeasure * 100 for k in rouge_scores}\n",
    "\n",
    "def compute_custom_metrics(srcs, golds, preds, device):\n",
    "    scores = defaultdict(list)\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "    sari = evaluate.load(\"sari\")\n",
    "    \n",
    "    # For rouge and length go over examples one by one and determine mean\n",
    "    for gold, pred in zip(golds, preds):\n",
    "        for k, v in get_rouge_score(gold, pred).items():\n",
    "            scores[k].append(v)\n",
    "        scores['words'].append(len(pred.split(' ')))\n",
    "    for k, v in scores.items():\n",
    "        scores[k] = np.mean(v)\n",
    "\n",
    "    # This is the default call using model_type=\"roberta-large\"\n",
    "    # This is the same as in the paper \"Generation of Patient After-Visit Summaries to Support Physicians\" (AVS_gen/eval_summarization.py) using the libary SummerTime\n",
    "    scores['bert_score'] = np.mean((bertscore.compute(predictions=preds, references=golds, lang=\"en\", device=device))['f1']) * 100\n",
    "    # BERTScore authors recommend \"microsoft/deberta-large-mnli\" (https://github.com/Tiiiger/bert_score)\n",
    "    scores['bert_score_deberta-large'] = np.mean((bertscore.compute(predictions=preds, references=golds, device=device, model_type=\"microsoft/deberta-large-mnli\"))['f1']) * 100\n",
    "    scores['sari'] = sari.compute(sources=srcs, predictions=preds, references=[[g] for g in golds])['sari']\n",
    "    # scores['sari'] = scores['sari'][0]\n",
    "    # Importing readability for dallc score not working: https://pypi.org/project/py-readability-metrics/    \n",
    "\n",
    "    return scores\n",
    "\n",
    "def print_metrics_as_latex(metrics):\n",
    "    # Print latex table row\n",
    "    order = ['rouge1', 'rouge2', 'rouge3', 'rouge4', 'rougeL', 'bert_score', 'bert_score_deberta-large', 'sari', 'words']\n",
    "    print(' & '.join([f'${metrics[k]:.2f}$' for k in order]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files\n",
    "# test_data_file = \"/home/s_hegs02/patient_summaries_with_llms/gpt-4/summarization_data/exp_4_test.json\"\n",
    "# validation_examples = \"2_short_BHC_summary_prediction/valid_4000_600_chars.json\"\n",
    "# preds_data_file = \"/home/s_hegs02/patient_summaries_with_llms/data/hallucination_evaluation/gpt-4_exp4_results_3shot.jsonl\"\n",
    "# preds_data_file = \"/home/s_hegs02/patient_summaries_with_llms/data/qualitative_evaluation/gpt-4_exp6_results_3shot.jsonl\"\n",
    "\n",
    "# Experiment 1 and 2\n",
    "test_data_file = \"/home/s_hegs02/patient_summaries_with_llms/gpt-4/summarization_data/exp_1_test.json\"\n",
    "# preds_data_file = \"/home/s_hegs02/patient_summaries_with_llms/gpt-4/performance_results/gpt-4_exp1_results_prompt3.1_0shot.jsonl\"\n",
    "preds_data_file = \"/home/s_hegs02/patient_summaries_with_llms/gpt-4/performance_results/gpt-4_exp1_results_prompt3_5shot.jsonl\"\n",
    "# test_data_file = \"/home/s_hegs02/patient_summaries_with_llms/gpt-4/summarization_data/exp_2_test.json\"\n",
    "# preds_data_file = \"/home/s_hegs02/patient_summaries_with_llms/gpt-4/performance_results/gpt-4_exp2_results_prompt3.1_0shot.jsonl\"\n",
    "# preds_data_file = \"/home/s_hegs02/patient_summaries_with_llms/gpt-4/performance_results/gpt-4_exp2_results_prompt3_5shot.jsonl\"\n",
    "\n",
    "\n",
    "\n",
    "# Read jsonl files\n",
    "def read_jsonl(file_name):\n",
    "    with open(file_name, \"r\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "    \n",
    "# Read jsonl files\n",
    "test_data = read_jsonl(test_data_file)\n",
    "preds_data = read_jsonl(preds_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print included valid examples with indices\n",
    "for i in range(0, 3):\n",
    "    print(i)\n",
    "    print(test_data[i][\"text\"])\n",
    "    print(test_data[i][\"summary\"])\n",
    "    print(preds_data[i][\"summary\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics rounded:\n",
      "{'rouge1': 38.8, 'rouge2': 10.78, 'rouge3': 3.55, 'rouge4': 1.12, 'rougeL': 21.98, 'words': 131.86, 'bert_score': 86.67, 'bert_score_deberta-large': 61.3, 'sari': 42.88}\n",
      "$38.80$ & $10.78$ & $3.55$ & $1.12$ & $21.98$ & $86.67$ & $61.30$ & $42.88$ & $131.86$\n"
     ]
    }
   ],
   "source": [
    "srcs = [e[\"text\"] for e in test_data]\n",
    "golds = [e[\"summary\"] for e in test_data]\n",
    "preds = [e[\"summary\"] for e in preds_data]\n",
    "metrics_test = compute_custom_metrics(srcs, golds, preds, \"cuda\")\n",
    "\n",
    "metrics_test = {k: round(v, 2) for k, v in metrics_test.items()}\n",
    "print(\"Test metrics rounded:\")\n",
    "print(metrics_test)\n",
    "print_metrics_as_latex(metrics_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avs_gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
